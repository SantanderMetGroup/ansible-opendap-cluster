\documentclass[a4paper,12pt]{article}

\usepackage{graphicx}
\usepackage{float}

\begin{document}

% \title{THREDDS Clustering}
% \maketitle

% \setlength\parindent{0pt}

\section{Introduction}

The THREDDS project is developing middleware to bridge the gap between data providers and data users. The goal is to simplify the discovery and use of scientific data and to allow scientific publications and educational materials to reference scientific data. Due to THREDDS's lack of horizontal scalability and automatic configuration management and deployment, this service usually deals with service downtimes and time consuming configuration tasks, mainly when an intensive use is done as is usual within the scientific community (e.g. climate).

This project aims to improve the scalability of the THREDDS Data Server through the implementation of a cluster of TDS instances that are deployed in the backend and that are only visible from the outside through a reverse proxy in the frontend. This project also aims to improve the management of the TDS catalogs by partitioning the hierarchy of catalogs into multiple TDS instances that are deployed in the backend, allowing high availability of the datasets and tackling the current problem of large waiting times after performing a THREDDS reboot when publishing new catalogs.  

Instead of the classic installation and configuration of a single or multiple independent THREDDS servers, manually configured, this work presents an automatic provisioning, deployment and orchestration cluster of THREDDS servers.

\subsection{Ansible - Automatic deployment and configuration}

Ansible is a tool for automating configuration management and deployment and is used in TDS Clustering in order to automate the creation of the infrastructure.

This solution is based on Ansible playbooks, used to automate the deployment and management of the agents that conform the cluster. The playbooks are based on modules (or roles) of different backends and frontends load balancing setups and solutions. This implementation allows to configure different infrastructure and deployment setups, as more workers are easily added to the cluster by simply declaring them as Ansible variables and executing the playbooks.

\subsection{HTTP Load balancing}

The frontend load balancing system enables horizontal scalability by delegating requests to backend workers, consisting in a variable number of instances for the THREDDS server that are deployed inside Apache Tomcat containers. Through the clustering capacity of the Apache Tomcat server, in combination with the JK connector and the Apache Web Server, all HTTP features such as HTTP sessions, are available for the THREDDS cluster. This clustering also provides fault-tolerance and better reliability since if any of the workers fail another instance of the cluster can take over it.

Additionally to the Apache httpd+mod\_jk setup for the gateway, this project provides alternatives to perform the load balancing in the reverse proxy, such us HAproxy and nginx.

\subsubsection{HTTP Load balancing caveats}

In the context of HTTP load balancing, the load balancer becomes a bottleneck, since all the traffic goes through it. This is particularly problematic when large datasets are accessed by the clients, because the load balancer must perform heavy transfers of data at the kernel level, which involves a lot of overhead in data being copied from the network interface and the hard disk. This breaks the load balancer at the CPU level because of the excessive amount of work that it has to do.

\subsection{Direct Server Return}

Direct Server Return is a type of load balancing in which the load balancer routes packets to the backends without changing anything in it but the destination MAC address. The backends process the requests and answer directly to the clients, without passing through the load-balancer.

This approach do away with the bottleneck created in the load balancer by the HTTP load balancing strategy, although it requires additional low level configuration and HTTP features are not available anymore.

\section{Deployment model}

The purpose of the deployment model is to define the common information needed by the different load balancing infrastructures. This information, defined using Ansible variables, will be used by the different roles and scenarios to perform their specific deployment of the THREDDS cluster. The following image provides a first overview of the deployment model.

\begin{figure}[h]
\includegraphics[width=1\textwidth]{images/overview.png}
\end{figure}

In the figure we can identify the following agents:

\begin{itemize}
\item Reverse proxy or gateway
\item Instances (THREDDS instances)
\item Collections
\item Replicas
\end{itemize}

All these elements are described in detail in the following sections.

\subsection{The gateway}

The gateway is the software that works as a reverse proxy and it is in charge of performing the load balancing, forwarding requests to the backend TDS instances deployed in Tomcat application servers. Multiple options are considered in this project to act as a gateway: httpd+mod\_jk, HAproxy, Linux Virtual Server and others.

\subsection{THREDDS instances}

Each TDS instance, or simply instance, corresponds to a Tomcat server process running the THREDDS web application in the backend hosts. Each host can run one or more instances. 

Each instance has the option to support any number of collections and each instance will respond only to requests targeting data available in the supported collections. This is done through the appropriate filtering of requests in the gateway.

Instances are clustered in the backend using Tomcat's clustering capabilities, which allows user session replication and in case that one of the instances become unavailable, another can take over it without any disruption of the service.

\subsection{Collections and replicas}

Collections are hierarchical structures compounded of a root THREDDS catalog and it's referenced catalogs. You build a collection by creating a catalog.xml file following the THREDDS Client Catalog Specification (see https://www.unidata.ucar.edu/software/thredds/current/tds/catalog/InvCatalogSpec.html#service).

\begin{figure}[h]
\includegraphics[width=1\textwidth]{images/catalogs.png}
\end{figure}

The catalogs that form the collection will be copied into the TDS instances defined by the deployment model, inside the content/thredds directory. Also, the root catalog of the collection will be referenced by the root catalog of the TDS instance. This allows the TDS instance to serve multiple collections by inserting multiple <catalogRef> elements into the TDS instance's root catalog.

\subsection{How to define the deployment model}

The deployment model is defined using the following variables:

\begin{itemize}
\item[-] collections
\item[-] tds\_instances
\item[-] datasets
\end{itemize}

The following is an example of the definition of these variables:

\begin{lstlisting}
collections:
  - path: collection1 # Id of the collection and path in THREDDS
    top: True # Add a reference in the tds gateway to the collection
    catalogs:
      src: data/collection1
    services: 
      - base: /thredds/catalog
      - base: /thredds/fileServer
      - base: /thredds/dodsC

  - path: collection2
    top: True
    catalogs:
      src: data/collection2
    services:
      - base: /thredds/dap4
      - base: /thredds/dodsC
      - base: /thredds/catalog
      - base: /thredds/fileServer
      - base: /thredds/ncss/grid

datasets:
  - src: data/datasets
    dest: '' # If empty, path is public/ in THREDDS
  - src: /home/user/tests/ncml-dataset
    dest: '/path/to/dest'

tds:
  - name: instance1
    shutdown: 18000
    tds_debug: # optional, used to enable jpda and other debug features
      jpda_address: "{{ ansible_eth1.ipv4.address }}:8000"
    connectors: # optional, connectors in conf/server.xml
      - protocol: HTTP/1.1
        port: 8080
    replicas: # collections replicated by this tds instance
      - proxy: proxy # proxy's ansible inventory name 
        host: "{{ ansible_eth1.ipv4.address }}" 
        collections: 
          - collection1
          - collection2
        properties: # <Connector> in conf/server.xml
          port: 18009
          protocol: AJP/1.3
\end{lstlisting}

\section{Roles and scenarios}

\subsection{Overview of roles}

Ansible roles have been created to split the deployment of TDS Clustering into reusable parts. A user who wants to implement his specific TDS Clustering scenario has to define an Ansible inventory and a playbook, and also define the variables that the Ansible roles will use to do the deployment.

The defined Ansible roles are:

\begin{itemize}
\item[-] ansible-miniconda-role - Role to install miniconda
\item[-] virtualenv - Role to install a python virtualenv
\item[-] virtualenv-conda - Role to install a virtualenv using miniconda
\item[-] supervisord - Role to install supervisord in top of a virtualenv
\item[-] httpd - Role to install httpd from source code
\item[-] httpd-bin - Role to install httpd from system packages
\item[-] jk-gateway - Role to deploy the mod\_jk gateway in top of httpd
\item[-] jk-gateway-tds - Role to deploy collections and replicas in top of mod\_jk
\item[-] tomcat - Role to deploy a tomcat server that will contain multiple instances
\item[-] tds - Role to deploy the THREDDS Data Server in tomcat instances
\item[-] tds-cluster - Role to deploy common cluster configuration
\item[-] tds-modjk - Bridge between jk-gateway and tds
\end{itemize}

\begin{figure}[h]
\includegraphics[width=1\textwidth]{images/roles.png}
\end{figure}

Roles are further explained in the following chapters.

\subsection{Overview of scenarios}

Scenarios are concrete use cases that make use of the available Ansible roles to do deployments with specific settings. Roles do the deployment of the minimum infrastructure for the THREDDS cluster to run and any customization must be defined in the scenarios.

Any scenario will be composed of:

\begin{itemize}
\item[-] inventory - Ansible inventory file, which defines the hosts involved in the deployment.
\item[-] main playbook - Ansible playbook that will contain two plays, one for the gateway hosts and other for the tds instances hosts, plus an optional third play that is in charge of the provisioning of the hosts.
\item[-] ansible.cfg - Ansible configuration file.
\item[-] user variables - Variables defined by the user that are required by the roles or that are defined to customize the deployment of the scenario.
\end{itemize}

\section{Apache httpd+mod\_jk gateway}

When the gateway is built using the Apache httpd+mod\_jk setup, it relies on the binary Apache JServ protocol to perform the load balancing. For each collection declared by the user through Ansible vars, a mod\_jk worker of type lb is created in the gateway. This worker of type lb is populated with Tomcat workers based on the attribute replicas of the tds\_instances Ansible var defined by the user.

In this context, collections are mod\_jk workers of type load balancing compounded of replicas, that is, mod\_jk balance workers. Each replica points to a tomcat instance deployed in the backend hosts that holds a copy of the contents served by the collection. Each request is routed, through the use of urimaps, to the TDS instances that support the collection in which the requested data resides.

Connectors of type AJP are used to connect the gateway with the instances. Usually, you only need one Connector element per gateway in the deployment, since Connector elements must reference their proxies. Each instance can contain one or more connectors (Connector xml elements in Tomcat's server.xml).

\begin{figure}[h]
\includegraphics[width=1\textwidth]{images/jk-gateway.png}
\end{figure}

Additionally to the mod\_jk workers described above, a mod\_jk status worker is also created, which allows the user to check the state of the rest of the workers in a html view. See the role reference for more information.

\subsection{Implementation example}

The httpd+mod\_jk implementation of THREDDS Clustering uses the information provided by the deployment model to create the appropriate mod\_jk workers and urimaps files, in addition to create the appropriate <Connector> elements in the tds instances (which will be of type AJP in this case).

We are going to see the workers and urimaps files created given the following deployment model:

\begin{lstlisting}
collections:
  - path: collection1 
    top: True 
    catalogs:
      src: data/collection1
    services: 
      - base: /thredds/catalog
      - base: /thredds/fileServer
      - base: /thredds/dodsC

datasets:
  - src: data/datasets
    dest: '' 

tds:
  - name: instance1
    shutdown: 18000
    replicas: 
      - proxy: proxy 
        host: server1
        collections: 
          - collection1
        properties: 
          port: 18009
          protocol: AJP/1.3
\end{lstlisting}

The mod\_jk urimaps file will look like the following:

\begin{lstlisting}
/thredds/catalog/collection1=COLLECTION_collection1
/thredds/catalog/collection1/*=COLLECTION_collection1
/thredds/fileServer/collection1=COLLECTION_collection1
/thredds/fileServer/collection1/*=COLLECTION_collection1
/thredds/dodsC/collection1=COLLECTION_collection1
/thredds/dodsC/collection1/*=COLLECTION_collection1
\end{lstlisting}

The mod\_jk workers file will look like the following:

\begin{lstlisting}
worker.server1_18009.type=ajp13
worker.server1_18009.host=server1
worker.server1_18009.port=18009

worker.list=COLLECTION_collection1
worker.COLLECTION_collection1.type=lb
worker.COLLECTION_collection1.mount=/thredds/restrictedAccess/*

worker.COLLECTION_collection1.balance_workers=server_18009
\end{lstlisting}

In this configuration, a collection named collection1 is created in the gateway and it is replicated in the backend host running a THREDDS instance. The THREDDS instance provides a Connector listening in the specified port where the gateway can connect in order to forward requests. The worker names for the THREDDS instances are compounded of the hostname of the host where they are running plus the port in which the AJP connector is listening, allowing multiple instances to listen in the same host.

\end{document}
